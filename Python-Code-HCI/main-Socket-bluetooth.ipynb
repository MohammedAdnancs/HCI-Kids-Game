{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import asyncio\n",
    "from bleak import BleakScanner\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from bluetooth import discover_devices, BluetoothSocket, RFCOMM\n",
    "from roboflow import Roboflow\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import keyboard\n",
    "import select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'DESKTOP-28SQ46K'    \n",
    "port = 8000\n",
    "message_to_client = None\n",
    "message_from_clinet = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def detect_objects_on_frame(model, frame, conf_threshold=0.5):\n",
    "\n",
    "    results = model.predict(source=frame, conf=conf_threshold, verbose=False)\n",
    "\n",
    "    detections = []  # To store detected objects\n",
    "    # Extract detection results and draw on the frame\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())  # Bounding box coordinates\n",
    "        confidence = box.conf[0].item()  # Confidence score\n",
    "        class_id = int(box.cls[0].item())  # Class ID\n",
    "        class_name = model.names[class_id]  # Class name\n",
    "\n",
    "        # Append the detected object details to the list\n",
    "        detections.append({\n",
    "            \"class_name\": class_name,\n",
    "            \"confidence\": confidence,\n",
    "            \"bbox\": [x1, y1, x2, y2]\n",
    "        })\n",
    "\n",
    "        # Draw bounding box and label on the frame\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box\n",
    "        label = f\"{class_name} {confidence:.2f}\"\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send \"pinching\" message to the server\n",
    "async def pinching_status_callback(pinching_detected,pinch_coordinates=None):\n",
    "    if pinching_detected:\n",
    "        if pinch_coordinates:\n",
    "            return(f\"Pinching:true,{pinch_coordinates[0]},{pinch_coordinates[1]}\")\n",
    "    # else:\n",
    "        # print(\"Not pinching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getPointsRealTime(pinching_detected_callback,frame):\n",
    "\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    mp_hands = mp.solutions.hands\n",
    "    drawing_utils = mp.solutions.drawing_utils\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "            # Flip the frame horizontally to mirror the camera feed\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            results = holistic.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw hand landmarks\n",
    "            drawing_utils.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            drawing_utils.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "            # Check for pinch gesture\n",
    "            pinching_detected = False\n",
    "            pinch_coordinates = None\n",
    "            if results.right_hand_landmarks or results.left_hand_landmarks:\n",
    "                try:\n",
    "                    hand_landmarks = results.right_hand_landmarks or results.left_hand_landmarks\n",
    "                    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "                    \n",
    "                    # Calculate distance between thumb and index fingertip to detect pinch\n",
    "                    distance = ((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2) ** 0.5\n",
    "\n",
    "                    if distance < 0.08:  # Adjust threshold as needed\n",
    "                        pinching_detected = True\n",
    "                        # Calculate pinch coordinates\n",
    "                        pinch_x = int(index_tip.x * image.shape[1])\n",
    "                        pinch_y = int(index_tip.y * image.shape[0])\n",
    "                        pinch_coordinates = (pinch_x, pinch_y)\n",
    "                        # Draw a green dot at the pinch location\n",
    "                        cv2.circle(image, (pinch_x, pinch_y), 10, (0, 255, 0), -1)  # Draw green dot\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing hand landmarks: {e}\")\n",
    "\n",
    "            return await pinching_detected_callback(pinching_detected,pinch_coordinates)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def receive_data(client_socket):\n",
    "    while True:\n",
    "        \n",
    "        data = await asyncio.to_thread(client_socket.recv, 1024)\n",
    "        global message_from_clinet\n",
    "        message_from_clinet = data.decode()\n",
    "\n",
    "        if data:\n",
    "            print(f\"Received from client: {data.decode()}\")\n",
    "        else:\n",
    "            print(\"Client disconnected\")\n",
    "            break\n",
    "\n",
    "async def send_data_to_client(client_socket, message_queue):\n",
    "    while True:\n",
    "        # Wait for a message from the queue\n",
    "        message_to_client = await message_queue.get()\n",
    "        if message_to_client:\n",
    "            #print(f\"Sending message to client: {message_to_client}\")  # Add logging for sent messages\n",
    "            client_socket.sendall(message_to_client.encode())\n",
    "        message_queue.task_done()  # Mark the message as processed\n",
    "\n",
    "async def start_server(host, port, message_queue):\n",
    "    # Create a TCP/IP socket\n",
    "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    \n",
    "    # Bind the socket to the address and port\n",
    "    server_socket.bind((host, port))\n",
    "    \n",
    "    # Enable the server to listen for incoming connections\n",
    "    server_socket.listen(5)\n",
    "    print(f\"Server listening on {host}:{port}...\")\n",
    "    \n",
    "    # Wait for a client to connect\n",
    "    client_socket, client_address = await asyncio.to_thread(server_socket.accept)\n",
    "    print(f\"Client connected: {client_address}\")\n",
    "    \n",
    "    try:\n",
    "        # Start both receiving and sending concurrently\n",
    "        await asyncio.gather(\n",
    "            receive_data(client_socket),\n",
    "            send_data_to_client(client_socket, message_queue)\n",
    "        )\n",
    "    finally:\n",
    "        # Clean up the connection\n",
    "        client_socket.close()\n",
    "        server_socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_sockets = {}\n",
    "# Initialize MediaPipe Hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def track_hand_and_get_index_finger(frame):\n",
    "    \n",
    "    # Convert the image color to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    index_finger_coords = None\n",
    "    name = \"No hand detected\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get the x and y coordinates of the index finger tip (landmark 8)\n",
    "            index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "            # Convert the normalized coordinates to pixel values\n",
    "            h, w, c = frame.shape\n",
    "            index_x = int(index_finger_tip.x * w)\n",
    "            index_y = int(index_finger_tip.y * h)\n",
    "\n",
    "            # Draw a circle at the index finger tip\n",
    "            cv2.circle(frame, (index_x, index_y), 10, (255, 0, 0), -1)\n",
    "            \n",
    "            # Determine if it's a left or right hand based on landmarks\n",
    "            if hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x < hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP].x:\n",
    "                name = \"Left hand\"\n",
    "            else:\n",
    "                name = \"Right hand\"\n",
    "                \n",
    "            index_finger_coords = (index_x, index_y)\n",
    "\n",
    "    return name, index_finger_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def initialize_emotion_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    # Load the trained weights\n",
    "    await asyncio.to_thread(model.load_weights, 'model.h5')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def predict_emotion(frame, model, emotion_dict):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize the face cascade for face detection\n",
    "    facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = facecasc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # Process the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        \n",
    "        # Extract the region of interest (ROI) for emotion prediction\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "\n",
    "        # Suppress TensorFlow/Keras verbosity\n",
    "        tf.get_logger().setLevel('ERROR')  # Suppress verbose logging\n",
    "\n",
    "         # If model is async, we need to await it\n",
    "        if asyncio.iscoroutinefunction(model.predict):\n",
    "            prediction = await model.predict(cropped_img, verbose=0)\n",
    "        else:\n",
    "            # If it's not async, use asyncio.to_thread to run it in a separate thread\n",
    "            prediction = await asyncio.to_thread(model.predict, cropped_img, verbose=0)\n",
    "        \n",
    "        maxindex = int(np.argmax(prediction))\n",
    "\n",
    "        # Return the predicted emotion label\n",
    "        return emotion_dict[maxindex]\n",
    "    \n",
    "    return \"No Face\"  # If no face is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def face_recognition(frame, recognizer, faceCascade, names):\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return \"No Face\", None  # Return default values if no faces are detected\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        \n",
    "        # Recognize the face using the trained model\n",
    "        id, confidence = recognizer.predict(gray[y:y + h, x:x + w])\n",
    "\n",
    "        if confidence > 10:\n",
    "            name = names[id]\n",
    "        else:\n",
    "            name = \"Unknown\"\n",
    "\n",
    "        # Return the name and the coordinates of the face\n",
    "        return name, (x, y, w, h)\n",
    "    \n",
    "    return \"No Face\", None  # In case the loop ends without detecting a face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on DESKTOP-28SQ46K:8000...\n",
      "Client connected: ('192.168.1.6', 62926)\n",
      "Received from client: Hello, Server!\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Received from client: face_recogention\n",
      "Client disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-2172' coro=<start_server() done, defined at C:\\Users\\medoa\\AppData\\Local\\Temp\\ipykernel_18996\\4168792974.py:23> exception=ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\medoa\\AppData\\Local\\Temp\\ipykernel_18996\\4168792974.py\", line 40, in start_server\n",
      "    await asyncio.gather(\n",
      "  File \"C:\\Users\\medoa\\AppData\\Local\\Temp\\ipykernel_18996\\4168792974.py\", line 20, in send_data_to_client\n",
      "    client_socket.sendall(message_to_client.encode())\n",
      "ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Exiting Program.\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    message_queue = asyncio.Queue()  # Create a queue to send messages to the server\n",
    "    \n",
    "    socket_task = asyncio.create_task(start_server(host, port,message_queue))\n",
    "    # Create LBPH Face Recognizer\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    # Load the trained model\n",
    "    recognizer.read('trainer.yml')\n",
    "    modelObject = YOLO('C:/Users/medoa/Desktop/HCI-Project-master/Python-Code-HCI/object detection/runs/detect/custom_yolo_model/weights/best.pt')  # Update with your model's path\n",
    "\n",
    "    # Path to the Haar cascade file for face detection\n",
    "    face_cascade_Path = \"haarcascade_frontalface_default.xml\"\n",
    "    # Create a face cascade classifier\n",
    "    faceCascade = cv2.CascadeClassifier(face_cascade_Path)\n",
    "\n",
    "    # Initialize user IDs and associated names\n",
    "    names = ['None']\n",
    "    is_pinching=None\n",
    "    with open('names.json', 'r') as fs:\n",
    "        names = json.load(fs)\n",
    "        names = list(names.values())\n",
    "    \n",
    "    # Initialize emotion model\n",
    "    emotion_model = await initialize_emotion_model()\n",
    "    \n",
    "    # Define emotion dictionary\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # Video Capture from the default camera (camera index 0)\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    cam.set(3, 640)  # Set width\n",
    "    cam.set(4, 480)  # Set height\n",
    "\n",
    "    # Create the window\n",
    "    cv2.namedWindow('camera', cv2.WINDOW_NORMAL)\n",
    "    # Resize the window\n",
    "    cv2.resizeWindow('camera', 1700, 980)  # Change the values as needed (e.g., 320, 240 for a smaller window)\n",
    "    last_sent_time = time.time()  # Initialize the timer\n",
    "    prev_time = time.time()\n",
    "\n",
    "    emotion = \"unkown\"\n",
    "    index_finger_coords = \"unkown\"\n",
    "    Object_Detected = \"unkown\"\n",
    "    face_coords = \"unkown\"\n",
    "    is_logged_in = False\n",
    "    lgoin_face = False\n",
    "    login_bluetooth = False\n",
    "    use_hand_index = False\n",
    "    Object_detection = False\n",
    "    is_playing = False\n",
    "    name = \"unkown\"\n",
    "    is_pinching = \"unkown\"\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        global message_from_clinet\n",
    "        \n",
    "        if(message_from_clinet == \"face_recogention\"):\n",
    "            lgoin_face = True\n",
    "\n",
    "        if(message_from_clinet == \"not_face_recogention\"):\n",
    "            lgoin_face = False\n",
    "\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cam.read()\n",
    "        \n",
    "\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        fps = int(fps)  # Make it a whole number\n",
    "        prev_time = current_time\n",
    "\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Failed to read from the camera.\")\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame,1)\n",
    "        resized_frame = cv2.resize(frame, (224, 224))\n",
    "        # Call face recognition function\n",
    "        \n",
    "        if(lgoin_face):\n",
    "            name, face_coords = await asyncio.create_task(face_recognition(resized_frame, recognizer, faceCascade, names))\n",
    "            \n",
    "        \n",
    "        index_finger_coords = await asyncio.create_task(track_hand_and_get_index_finger(frame))\n",
    "\n",
    "        \n",
    "        #is_pinching = await asyncio.create_task(getPointsRealTime(pinching_status_callback,resized_frame))\n",
    "\n",
    "        if(Object_detection):\n",
    "            Object_Detected = await asyncio.create_task(detect_objects_on_frame(modelObject,resized_frame))\n",
    "\n",
    "\n",
    "        # Display the frame with landmarks and index finger tip\n",
    "        if index_finger_coords and isinstance(index_finger_coords, tuple) and len(index_finger_coords) == 2:\n",
    "            x, y = index_finger_coords\n",
    "            # Ensure that x and y are integers\n",
    "            if isinstance(x, int) and isinstance(y, int):\n",
    "                cv2.circle(frame, (x, y), 10, (128, 0, 128), 3)  # Draw the circle\n",
    "       \n",
    "        # Call emotion prediction function\n",
    "        if(is_playing):\n",
    "            emotion = await predict_emotion(resized_frame, emotion_model, emotion_dict)\n",
    "\n",
    "        cv2.putText(frame, f\"FPS : {fps}\", (5, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame, f\"Name : {name}\", (5, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame, f\"Emotion : {emotion}\", (5, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame, f\"IndexFinger : {index_finger_coords}\", (5, 170), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame,f\"Pinching : {is_pinching}\", (5, 230), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        cv2.putText(frame, f\"Object : {Object_Detected}\", (5, 290), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2)\n",
    "        # Check if 5 seconds have passed since the last recognition\n",
    "        \n",
    "        current_time = time.time()\n",
    "        \n",
    "        if current_time - last_sent_time >= 0.01:\n",
    "            #print(f\"Recognized: {name}, Emotion: {emotion},Object_Detected:{Object_Detected},Pinching:{is_pinching},Index Finger coordinates: {index_finger_coords}\")\n",
    "            last_sent_time = current_time  # Update the timer\n",
    "         # Send the message to the server via the queue\n",
    "            message_to_client = f\" {name or 'None'},{emotion or 'None'},{Object_Detected or 'None'},{is_pinching or 'None'},{index_finger_coords or 'None'}\"\n",
    "            await asyncio.create_task (message_queue.put(message_to_client) ) # Put the message in the queue\n",
    "\n",
    "        # Display the image with rectangles around faces and objects\n",
    "\n",
    "        cv2.imshow('camera', frame)\n",
    "        \n",
    "        # Press Escape or 'q' to exit the webcam/program\n",
    "        k = cv2.waitKey(5) & 0xff\n",
    "        if k == 27 or k == ord('q'):  # 27 is the Escape key; ord('q') checks for 'q'\n",
    "            break\n",
    "        \n",
    "\n",
    "    print(\"\\n [INFO] Exiting Program.\")\n",
    "    # Release the camera\n",
    "    cam.release()\n",
    "    # Close all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
